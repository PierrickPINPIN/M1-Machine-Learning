---
title: "PW3"
output: html_document
date: "2022-09-29"
---

# **Multiple Linear Regression**

Question 1
```{r}
library(MASS)

dim(Boston)

summary(Boston)

View(Boston)
```

Question 2
```{r}
train= 1:round(dim(Boston)[1]*0.8, digits=0)

test= -train

training_set= Boston[train,]

test_set= Boston[test,]

View(training_set)
View(test_set)
```
Compute the 80% of the length of the Boston dataset and split it with this ratio (80% for train_set and 20% for the test_set)

Question 3
```{r}
cor(x = training_set$medv, y= training_set$age)
```
```{r}
plot(training_set$medv, training_set$age)
```
We can see a correlation coefficient of -0.29. So we can conclude that there is no linearity between the variables 'medv' and 'age'. Moreover, the plot confirms it, graphically, we can't see any linearity

Question 4
```{r}
lm <- lm(medv~age, data= training_set)

plot(lm)
```
We can see on the Residuals vs Fitted and the Scale location plots that there is no linearity between the median value of the owner-occupied homes and the age of population.
With the Normal Q-Q diagram, we can conclude that the model is not relevant for this data.
There are not any influential points in our regression model.

Question 5 & 6
```{r}
training_set$lstat= log(training_set$lstat)

model1 <- lm(medv~lstat+age, data= training_set)

summary(model1)
```
The obtained model is a multiple linear regression: y= b0 + b1* X1 + b2* X2
Here we have b0= 42.33279; b1= -27.74774; b2= 0.05146.

Question 7

According to the Signif. codes (***), the predictors are significant.

Question 8

The model as a whole is significant. Indeed, we have a Residual standard error of 5.388. Moreover, the Adjusted R-squared is 0.6631. So 66.31% of the housing price can be explain by the variables lstat and age. The Multiple R-squared measure how well the model is fitting the data, but it doesn't take account of the number of the considered variable. While Adjusted R-squared does.That is why a large difference between Multiple R-squared and Adjusted R-squared indicates an overfited model. Here, the difference is very small.

Question 9
```{r}
model2 <- lm(medv~., data= training_set)
summary(model2)
```

Question 10
```{r}
training_set$lstat= log(training_set$lstat)
model3 <- lm(medv~., data= training_set)
summary(model3)
```

Question 11

We can observe:
- Adjusted R-squared of model1 (variables= lstat, age): 0.6732
- Adjusted R-squared of model2 (variables= all): 0.7798
- Adjusted R-squared of model3 (variables= all with log(lstat)): 0.7835 
So we can see that RÂ² improve with this model and the best is model3. Indeed, 78.35% of the mdv can be explain by the variables

Question 12
```{r}
Q <- round(cor(training_set, training_set), digits= 2)
Q
```

```{r}
for (i in 1:dim(Q)[1]) {
  for (j in 1:dim(Q)[2]) {
    if(Q[i,j] > 0.8 && rownames(Q)[i] != colnames(Q)[j])
    {
      ch <- paste(rownames(Q)[i], " and ", colnames(Q)[j], "are correlated: ", Q[i,j])
      print(ch)
    }
  }
}
```
So we can see that there are some correlations between the variables.

Question 13
```{r}
library(corrplot)
corrplot.mixed(Q)
```
As we saw before, there are some correlations between some variabes.

Question 14
```{r}
for (i in 1:dim(Q)[1]) {
  for (j in 1:dim(Q)[2]) {
    if(rownames(Q)[i] == "tax" && colnames(Q)[j] == "rad")
    {
      ch <- paste(rownames(Q)[i], " and ", colnames(Q)[j], "are correlated: ", Q[i,j])
      print(ch)
    }
  }
}
```
As we seen before, we can see a strong correlation between tax and rad.

Question 15
```{r}
model4 <- lm(medv~.-tax, data= training_set)
summary(model4)
```
We can see that Adjusted R-squared= 0.777 which is a lower value than model3 (with 'tax' variable).
F-statistic= 118.3 which is a higher value than the model which include the 'tax' variable. So, that indicates a higher dispersion from the mean. Then the model is more significant without rad.


Question 16
```{r}
MSE <- mean(summary(model4)$residuals^2)
MSE
```
The lower the value for MSE, the more accurately a model is able to predict values.

Question 17
```{r}
str(training_set$chas)

table(training_set$chas)
```
We can see that there is 35 suburbs in this data set which bound the Charles river.

Question 18
```{r}
library(ggplot2)
colors= c("blue", "cyan4")

ggplot(training_set, aes(x= as.factor(chas), y= medv)) +
  geom_boxplot() +
  ggtitle("Boxplot of the Median value of houses with respect \n to the neighborhood to Charles river") +
  theme(plot.title = element_text(hjust = 0.5)) +
  labs(x= "Neighborhood to Charles river", y= "Median value of houses" )
```
We can see that the medians are almost equal. However, there is more variance in prices if they are in the suburbs along the Charles River.

Question 19
```{r}
aggregate(medv~chas, data= training_set, mean)
```
This function computes the average of 'chas' variable.
The average price of a suburban home not bordering the Charles River is: 23.73.
The average house price in the Charles River suburb is: 28.44.

Question 20
```{r}
ANOVAtest <- aov(medv~chas, training_set)
ANOVAtest
```
```{r}
summary(ANOVAtest)
```
We can see a p-value of 0.00399 (which is very low), which means that our model is significant. So, the variable 'chas' is significant: there is a significant difference of price between a housing in suburbs along the Charles river and another.

## **Qualitative predictors**

Question 21
```{r}
model5 <- lm(medv~chas+crim, data= training_set)
summary(model5)
```
In this model with the form: Y= b0 + b1 * X1 + b2 * X2, we can see that:
b0= 24.61889
b1 = 4.63230
b2= -0.43808
We can see that the Pr(>|t|) of 'chas' is quiet low so ce can conclude that the variable 'chas' is significant with this model. Moreover, we can see an Adjusted R-squared of 0.1105, so 11.05% of medv can be explained with this variables.

Question 22
```{r}
summary(model3) #This model was fitted before (Question 10). It take account all the variables
```
We can see a Pr(>|t|) of 0.053253 for 'chas' variable. This is a high value, it means that 'chas' variable is not significant in this model.

## **Interaction terms**

Question 23
```{r}
model6 <- lm(medv~lstat*age, data= training_set)
summary(model6)
```
We can see with the values of Pr(>|t|) that all the variables are significant.
Moreover, we have an Adjusted R-squared value of 0.6775 which is quiet good. We can compare this value with a model without first order interaction terms. The value of the Adjusted R-squared of the model1 (Question 5) is 0.6732. So with first order interaction terms, we improved our model with the variables 'lstat' and 'age'.

Question 24
```{r}
model7 <- lm(medv~.*., data= training_set)
summary(model7)
```
We can see an Adjusted R-squared value of 0.9152 which is really good. Indeed, compared to the model3 (Question 10) which take account all the variable but without first order interaction terms, we had a value of 0.7835. So with this tool, we get a big improvement. To conclude, with this new model, 91.52% of the median value of houses for 506 neighborhoods around Boston can be explained by our variables.